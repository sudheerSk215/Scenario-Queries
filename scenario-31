from pyspark import *
from pyspark import SparkConf, SparkContext
from pyspark.sql import *
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import *



spark = SparkSession.builder.getOrCreate()

# creating the dataframe

data = [("m1", "m1,m2", "m1,m2,m3", "m1,m2,m3,m4")]

df = spark.createDataFrame(data, ["col1", "col2", "col3", "col4"])
df.show()

# concating the dataframe into single column

contdf = df.withColumn("col", expr("concat(col1,'-',col2,'-',col3,'-',col4)")).drop(
    "col1", "col2", "col3", "col4"
)
contdf.show()

finaldf = contdf.selectExpr("explode(split(col,'-')) as col")
finaldf.show()

+----+-----+--------+-----------+
|col1| col2|    col3|       col4|
+----+-----+--------+-----------+
|  m1|m1,m2|m1,m2,m3|m1,m2,m3,m4|
+----+-----+--------+-----------+

+--------------------+
|                 col|
+--------------------+
|m1-m1,m2-m1,m2,m3...|
+--------------------+

+-----------+
|        col|
+-----------+
|         m1|
|      m1,m2|
|   m1,m2,m3|
|m1,m2,m3,m4|
+-----------+
