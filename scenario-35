from pyspark.sql import SparkSession
from pyspark.sql import *
from pyspark.sql.types import *
from pyspark.sql.functions import *

#creating the dataframe df1
data1 = [(1,'Jhon',17),(2,'Maria',20),(3,'Raj',None),(4,'Rachel',18)]
columns = ["id","name","age"]
df1 = spark.createDataFrame(data1,columns)
df1.show()


# Count null entries in each column
null_counts = df1.select([sum(col(c).isNull().cast("int")).alias(c) for c in df1.columns ])

null_counts.show()


#Remove the row with null entires and store them in a new dataframe named df2
df2 = df1.filter(col("age").isNull())
df2.show()

#create a new dataframe df3
data2 = [(1,'seatle',82),(2,'london',75),(3,'banglore',60),(4,'boston',90)]
columns2 = ["id","city","code"]

df3 = spark.createDataFrame(data2,columns2)
df3.show()

mergedf = df1.join(df3, df1["id"]==df3["id"],"inner").select(df1["id"],"name","age","city","code")
mergedf.show()



